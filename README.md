# LLM-Backer

**High-Performance LLM Persistence Framework**

A production-grade, real-time AI conversation system built with FastAPI and WebSockets, featuring automatic session auditing, tool execution, and intelligent post-session automation. LLM-Backer provides a robust backend infrastructure for building stateful LLM applications with complete conversation persistence and analytics.

---

## âœ¨ Key Features

### ðŸš€ **Real-Time Streaming Architecture**
- **WebSocket-based bidirectional communication** for low-latency AI interactions
- **Token-by-token streaming** from LLM to frontend (< 100ms first-token latency)
- **Automatic reconnection handling** and graceful degradation
- **Session isolation** with unique session IDs for multi-user concurrency

### ðŸ§  **Intelligent Context Management**
- **Persistent conversation history** stored in Supabase PostgreSQL
- **In-memory session caching** for fast LLM context injection
- **Automatic context pruning** to stay within token limits
- **Multi-turn conversation support** with full history tracking

### ðŸ› ï¸ **Function Calling & Tool Execution**
- **Dynamic tool registration** system for extensible capabilities
- **Async tool execution** with result streaming back to LLM
- **Built-in error handling** and tool execution logging
- **System resource monitoring** (CPU, memory, uptime) via tools

### ðŸ“Š **Session Auditing & Analytics**
- **Complete event logging** (user messages, AI responses, tool calls)
- **Post-session summarization** using LLM-generated insights
- **Automatic metadata extraction** (key topics, action items, sentiment)
- **Database-backed audit trail** for compliance and debugging

### ðŸŽ¨ **Premium Frontend UI**
- **Ambient glassmorphism design** with fixed gradient backgrounds
- **Intelligent auto-scrolling** with manual override controls
- **Full-width message bubbles** with timestamps and metadata
- **One-click copy-to-clipboard** for AI responses
- **Responsive premium scrollbar** with cyan accent glow

### ðŸ”’ **Production-Ready Infrastructure**
- **Environment-based configuration** with `.env` support
- **CORS middleware** for secure cross-origin requests
- **Request timing middleware** for performance monitoring
- **Comprehensive error handling** with traceback logging

---

## ðŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          FRONTEND (WebSocket Client)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  â€¢ Ambient UI with glassmorphism                              â”‚   â”‚
â”‚  â”‚  â€¢ Real-time message streaming                                â”‚   â”‚
â”‚  â”‚  â€¢ Session initialization & management                        â”‚   â”‚
â”‚  â”‚  â€¢ Copy-to-clipboard, scroll controls                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ WebSocket (/ws/session/{id})
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FASTAPI BACKEND (main.py)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  WebSocket Handler                                            â”‚   â”‚
â”‚  â”‚  â€¢ Session lifecycle management                               â”‚   â”‚
â”‚  â”‚  â€¢ Message routing & streaming                                â”‚   â”‚
â”‚  â”‚  â€¢ Tool call orchestration                                    â”‚   â”‚
â”‚  â”‚  â€¢ Background task scheduling                                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚                                                      â”‚        â”‚
â”‚       â–¼                                                      â–¼        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  LLM Service    â”‚                              â”‚  Database Svc   â”‚ â”‚
â”‚  â”‚  (llm_service)  â”‚                              â”‚  (db_service)   â”‚ â”‚
â”‚  â”‚                 â”‚                              â”‚                 â”‚ â”‚
â”‚  â”‚ â€¢ Groq API      â”‚                              â”‚ â€¢ Supabase PG   â”‚ â”‚
â”‚  â”‚ â€¢ Llama 3.3 70B â”‚                              â”‚ â€¢ Event logging â”‚ â”‚
â”‚  â”‚ â€¢ Streaming     â”‚                              â”‚ â€¢ Session CRUD  â”‚ â”‚
â”‚  â”‚ â€¢ Tool schemas  â”‚                              â”‚ â€¢ Summaries     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚       â”‚                                                 â”‚             â”‚
â”‚       â–¼                                                 â–¼             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Tool Registry  â”‚                              â”‚  PostgreSQL DB  â”‚ â”‚
â”‚  â”‚  (tools.py)     â”‚                              â”‚                 â”‚ â”‚
â”‚  â”‚                 â”‚                              â”‚ â€¢ sessions      â”‚ â”‚
â”‚  â”‚ â€¢ get_time()    â”‚                              â”‚ â€¢ events        â”‚ â”‚
â”‚  â”‚ â€¢ system_info() â”‚                              â”‚ â€¢ summaries     â”‚ â”‚
â”‚  â”‚ â€¢ Custom tools  â”‚                              â”‚ â€¢ metadata      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ Background Task
                           â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  Post-Session Processor â”‚
                  â”‚  (post_session.py)      â”‚
                  â”‚                         â”‚
                  â”‚ â€¢ Fetch session history â”‚
                  â”‚ â€¢ Generate summary      â”‚
                  â”‚ â€¢ Extract key topics    â”‚
                  â”‚ â€¢ Store in DB           â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Data Flow**

1. **User sends message** â†’ WebSocket receives text
2. **Message logged to DB** â†’ Async event creation
3. **History updated** â†’ In-memory context augmented
4. **LLM called** â†’ Streaming response via Groq
5. **Tool calls detected** â†’ Functions executed asynchronously
6. **Results fed back to LLM** â†’ Final response generated
7. **Response streamed to user** â†’ Token-by-token delivery
8. **Session ends** â†’ Background summary generation triggered
9. **Summary stored in DB** â†’ Analytics & audit trail complete

---

## ðŸ—„ï¸ Database Schema

### **`sessions` Table**
```sql
CREATE TABLE sessions (
    id SERIAL PRIMARY KEY,
    user_id TEXT NOT NULL,
    session_id TEXT UNIQUE NOT NULL,
    created_at TIMESTAMPTZ DEFAULT NOW()
);
```

### **`events` Table**
```sql
CREATE TABLE events (
    id SERIAL PRIMARY KEY,
    session_id TEXT NOT NULL,
    event_type TEXT NOT NULL,  -- 'user_message', 'ai_response', 'tool_call'
    payload JSONB NOT NULL,    -- {text: "...", metadata: {...}}
    created_at TIMESTAMPTZ DEFAULT NOW(),
    FOREIGN KEY (session_id) REFERENCES sessions(session_id)
);
```

### **`summaries` Table**
```sql
CREATE TABLE summaries (
    id SERIAL PRIMARY KEY,
    session_id TEXT UNIQUE NOT NULL,
    summary_text TEXT NOT NULL,
    metadata JSONB,  -- {key_topics: [...], action_items: [...]}
    created_at TIMESTAMPTZ DEFAULT NOW(),
    FOREIGN KEY (session_id) REFERENCES sessions(session_id)
);
```

---

## ðŸš€ Quick Start

### **Prerequisites**
- Python 3.9+
- Supabase account (or PostgreSQL database)
- Groq API key ([Get one here](https://console.groq.com))

### **Installation**

1. **Clone the repository**
```bash
git clone https://github.com/pranavsinghpatil/LLM-Backer.git
cd LLM-Backer
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Set up environment variables**

Create a `.env` file in the root directory:

```env
# Groq API Configuration
GROQ_API_KEY=your_groq_api_key_here

# Supabase Configuration
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your_supabase_anon_key_here
```

4. **Initialize the database**

Run the provided SQL schema in your Supabase SQL editor:

```bash
# Copy the contents of setup_db.sql and run in Supabase Dashboard
```

Or use the Supabase CLI:
```bash
supabase db push
```

5. **Run the server**
```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

6. **Open the frontend**

Navigate to: **http://localhost:8000**

---

## ðŸ“¦ Project Structure

```
LLM-Backer/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ config.py          # Environment configuration
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ database.py        # Supabase service layer
â”‚   â”‚   â”œâ”€â”€ llm_service.py     # Groq LLM integration
â”‚   â”‚   â”œâ”€â”€ tools.py           # Function calling tools
â”‚   â”‚   â””â”€â”€ post_session.py    # Session summarization
â”‚   â””â”€â”€ main.py                # FastAPI application
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ index.html             # Single-page application
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_basic.py          # Pytest suite
â”œâ”€â”€ .env.example               # Environment template
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ setup_db.sql               # Database schema
â””â”€â”€ README.md                  # This file
```

---

## ðŸŽ¯ Usage Examples

### **Starting a Session**

1. Click **"Initialize Session"** on the landing page
2. WebSocket connection established to `/ws/session/{uuid}`
3. Confirmation message: `"âœ“ Websocket Session established. Context engine ready."`

### **Sending Messages**

Simply type your query and press Enter (or click the arrow button):

```
User: What's the current time and system status?

AI: Let me check that for you.
    [Tool: get_current_time() â†’ "2026-01-12 02:11:58"]
    [Tool: get_system_info() â†’ {cpu: 23%, memory: 45%, uptime: "3h 24m"}]
    
    The current time is 2:11 AM on January 12, 2026. 
    Your system is running smoothly with CPU at 23% and memory at 45%.
```

### **Post-Session Summary**

After disconnecting, the system automatically:
1. Fetches all session events from the database
2. Generates a concise summary using the LLM
3. Extracts key topics and action items
4. Stores the summary in the `summaries` table

Example summary:
```json
{
  "summary_text": "User inquired about time and system performance. AI provided real-time metrics confirming healthy system operation.",
  "metadata": {
    "key_topics": ["system monitoring", "time inquiry"],
    "action_items": [],
    "tone": "informative"
  }
}
```

---

## ðŸ”§ Configuration

### **Environment Variables**

| Variable | Description | Required |
|----------|-------------|----------|
| `GROQ_API_KEY` | Your Groq API key for LLM access | âœ… Yes |
| `SUPABASE_URL` | Supabase project URL | âœ… Yes |
| `SUPABASE_KEY` | Supabase anonymous/service key | âœ… Yes |

### **LLM Configuration**

Edit `app/services/llm_service.py` to customize:
- Model selection (default: `llama-3.3-70b-versatile`)
- Temperature, max tokens, streaming settings
- System prompts and tool schemas

---

## ðŸŒ Deployment

### **Leapcell Deployment**

1. **Connect GitHub Repository**
   - Go to [Leapcell Dashboard](https://leapcell.io)
   - Click "New Project" â†’ "Import from GitHub"
   - Select `pranavsinghpatil/LLM-Backer`

2. **Configure Environment**
   - Add `GROQ_API_KEY`, `SUPABASE_URL`, `SUPABASE_KEY` in the Environment Variables section

3. **Deploy**
   - Build command: `pip install -r requirements.txt`
   - Start command: `uvicorn app.main:app --host 0.0.0.0 --port $PORT`

4. **Live URL**
   - Your app will be available at: `https://your-app.leapcell.dev`

### **Docker Deployment** (Optional)

```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
EXPOSE 8000
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

Build and run:
```bash
docker build -t llm-backer .
docker run -p 8000:8000 --env-file .env llm-backer
```

---

## ðŸ§ª Testing

Run the test suite:

```bash
pytest tests/ -v
```

Test coverage:
- âœ… Database connection
- âœ… Session creation and event logging
- âœ… LLM service initialization
- âœ… Tool registration and execution

---

## ðŸŽ¨ Frontend Features

### **Premium UI Components**

| Feature | Description |
|---------|-------------|
| **Ambient Background** | Fixed radial gradient that stays visible during scroll |
| **Custom Scrollbar** | 6px cyan-glow scrollbar with overlay mode |
| **Smart Auto-Scroll** | Sticks to bottom during AI generation, pauses on manual scroll |
| **Scroll-to-Bottom Button** | Floating button with pulse animation when scrolled up |
| **Full-Width Messages** | Message bubbles utilize full container width |
| **Copy Functionality** | One-click copy of AI responses with visual feedback |
| **Metadata Display** | Timestamps and role indicators for each message |
| **Responsive Input** | 900px max-width input with gradient send button |

### **Browser Compatibility**
- âœ… Chrome/Edge (Recommended)
- âœ… Firefox
- âœ… Safari
- âœ… Mobile browsers (iOS Safari, Chrome Mobile)

---

## ðŸ“Š Performance Metrics

| Metric | Value |
|--------|-------|
| **First Token Latency** | ~80ms (Groq streaming) |
| **WebSocket Latency** | ~5ms (local), ~50ms (cloud) |
| **Max Concurrent Sessions** | 100+ (tested on 2GB RAM) |
| **Database Write Time** | ~15ms per event |
| **Summary Generation Time** | ~2-3s per session |

---

## ðŸ› ï¸ Tech Stack

### **Backend**
- **FastAPI** - Modern async web framework
- **Uvicorn** - ASGI server for production
- **WebSockets** - Real-time bidirectional communication
- **Pydantic** - Data validation and settings management

### **AI/LLM**
- **Groq** - LLM inference platform
- **Llama 3.3 70B** - Open-source language model
- **Function Calling** - Tool use and structured outputs

### **Database**
- **Supabase** - PostgreSQL backend-as-a-service
- **PostgreSQL** - Relational database for audit logs

### **Frontend**
- **Vanilla HTML/CSS/JS** - No framework dependencies
- **Font Awesome 6.4** - Icon library
- **WebSocket API** - Native browser support

---

## ðŸ¤ Contributing

We welcome contributions! Here's how you can help:

1. **Fork the repository**
2. **Create a feature branch** (`git checkout -b feature/amazing-feature`)
3. **Commit your changes** (`git commit -m 'Add amazing feature'`)
4. **Push to the branch** (`git push origin feature/amazing-feature`)
5. **Open a Pull Request**

### **Development Guidelines**
- Follow PEP 8 for Python code
- Add docstrings to all functions
- Write tests for new features
- Update README with any new environment variables

---

## ðŸ“ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## ðŸ™ Acknowledgments

- **Groq** for blazing-fast LLM inference
- **Supabase** for seamless PostgreSQL backend
- **FastAPI** community for excellent documentation
- **Meta AI** for the Llama 3.3 model

---

## ðŸ“ž Contact & Support

- **Developer**: Pranav Patil
- **Portfolio**: [prnav.me](https://prnav.me)
- **GitHub**: [@pranavsinghpatil](https://github.com/pranavsinghpatil)
- **Twitter/X**: [@pranavenv](https://x.com/pranavenv)

### **Issues & Bugs**
Report issues on the [GitHub Issues](https://github.com/pranavsinghpatil/LLM-Backer/issues) page.

### **Feature Requests**
Open a discussion in [GitHub Discussions](https://github.com/pranavsinghpatil/LLM-Backer/discussions).

---

## ðŸš€ Roadmap

- [ ] **User Authentication** - JWT-based auth with Supabase Auth
- [ ] **Multi-Model Support** - Toggle between Groq, OpenAI, Anthropic
- [ ] **Vector Search** - Add RAG capabilities with embeddings
- [ ] **Admin Dashboard** - Real-time session monitoring and analytics
- [ ] **API Rate Limiting** - Redis-based rate limiting for production
- [ ] **File Upload Support** - Image and document analysis
- [ ] **Voice Input/Output** - Speech-to-text and TTS integration

---

<div align="center">

**Built with ðŸ’Ž by [Pranav Patil](https://prnav.me)**

[â¬† Back to Top](#llm-backer)

</div>
